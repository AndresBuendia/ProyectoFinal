{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to print metrics\n",
    "class MetricsLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"Epoch {epoch + 1}: loss = {logs.get('loss')}, val_loss = {logs.get('val_loss')}, \"\n",
    "              f\"mse = {logs.get('mse')}, val_mse = {logs.get('val_mse')}, \"\n",
    "              f\"mae = {logs.get('mae')}, val_mae = {logs.get('val_mae')}\")\n",
    "\n",
    "# Function to build the model based on the parameters\n",
    "def build_model(input_shape, units=[256, 128, 64], dropout_rate=0.2, recurrent_dropout_rate=0.2, optimizer='rmsprop', model_type='GRU'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for i, unit in enumerate(units):\n",
    "        if model_type == 'GRU':\n",
    "            model.add(GRU(unit, return_sequences=(i < len(units) - 1), dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate))\n",
    "        elif model_type == 'LSTM':\n",
    "            model.add(LSTM(unit, return_sequences=(i < len(units) - 1), dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate))\n",
    "        elif model_type == 'Bidirectional_LSTM':\n",
    "            model.add(Bidirectional(LSTM(unit, return_sequences=(i < len(units) - 1), dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))  # Capa de salida con una unidad para la predicción del valor objetivo\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to save metrics and hyperparameters\n",
    "def save_metrics_and_params(symbol, model_dir, best_score, best_params):\n",
    "    metrics_path = os.path.join(model_dir, f\"{symbol}_best_metrics.txt\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(f\"Best Score (MAE): {best_score}\\n\")\n",
    "        f.write(\"Best Hyperparameters:\\n\")\n",
    "        for param, value in best_params.items():\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "def train_and_evaluate(symbol, model_dir, param_grid, sequence_length):\n",
    "    print(f\"Entrenando y evaluando modelo para {symbol}\")\n",
    "\n",
    "    # Cargar los datos preparados\n",
    "    prepared_data_path = os.path.join(\"..//content/drive/My Drive/tradingcripto/data/processed\", f\"{symbol}_prepared_data.pkl\")\n",
    "    X_train, X_test, y_train, y_test, feature_scaler, target_scaler = joblib.load(prepared_data_path)\n",
    "\n",
    "    if check_overlap(X_train, X_test):\n",
    "        raise ValueError(f\"Existe solapamiento entre los conjuntos de entrenamiento y validación para {symbol}\")\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Definir la ruta para guardar el mejor modelo\n",
    "    symbol_model_dir = os.path.join(model_dir, symbol)\n",
    "    os.makedirs(symbol_model_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(symbol_model_dir, \"best_model.keras\")\n",
    "\n",
    "    # Configurar la validación cruzada\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # Variables para almacenar los mejores resultados\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model_path_temp = None\n",
    "\n",
    "    # Iterar sobre las combinaciones de hiperparámetros\n",
    "    for units in param_grid['units']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for optimizer in param_grid['optimizer']:\n",
    "                for model_type in param_grid['model_type']:\n",
    "                    for epochs in param_grid['epochs']:\n",
    "                        for batch_size in param_grid['batch_size']:\n",
    "                            val_losses = []\n",
    "\n",
    "                            # Validación cruzada\n",
    "                            for fold, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "                                X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "                                y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "                                # Definir el modelo\n",
    "                                model = build_model(input_shape, units=units, dropout_rate=dropout_rate, optimizer=optimizer, model_type=model_type)\n",
    "\n",
    "                                # Inicializar el callback para el checkpoint personalizado\n",
    "                                early_stopping = EarlyStopping(monitor='val_mae', mode='min', patience=6, verbose=1)\n",
    "                                metrics_logger = MetricsLogger()\n",
    "\n",
    "                                # Definir la ruta para guardar el modelo del fold actual\n",
    "                                fold_model_path = os.path.join(symbol_model_dir, f\"model_{units}_{dropout_rate}_{optimizer}_{model_type}_{epochs}_{batch_size}_fold_{fold}.keras\")\n",
    "                                checkpoint = ModelCheckpoint(fold_model_path, monitor='val_mae', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "                                try:\n",
    "                                    history = model.fit(\n",
    "                                        X_train_fold, y_train_fold,\n",
    "                                        epochs=epochs,\n",
    "                                        batch_size=batch_size,\n",
    "                                        validation_data=(X_val_fold, y_val_fold),\n",
    "                                        callbacks=[early_stopping, metrics_logger, checkpoint],\n",
    "                                        verbose=0\n",
    "                                    )\n",
    "\n",
    "                                    val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)[2]  # Get MAE only\n",
    "                                    val_losses.append(val_mae)\n",
    "\n",
    "                                except ConnectionResetError as e:\n",
    "                                    print(f\"Connection error encountered: {e}. Retrying...\")\n",
    "                                    continue\n",
    "\n",
    "                            mean_val_loss = np.mean(val_losses)\n",
    "                            print(f\"Mean validation loss (MAE) for {symbol} with {units} units, {dropout_rate} dropout, {optimizer} optimizer, {model_type} model, {epochs} epochs, and {batch_size} batch size: {mean_val_loss}\")\n",
    "\n",
    "                            # Guardar los mejores hiperparámetros y el mejor modelo según la media de val_mae\n",
    "                            if mean_val_loss < best_score:\n",
    "                                best_score = mean_val_loss\n",
    "                                best_params = {\n",
    "                                    'units': units,\n",
    "                                    'dropout_rate': dropout_rate,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'model_type': model_type,\n",
    "                                    'epochs': epochs,\n",
    "                                    'batch_size': batch_size\n",
    "                                }\n",
    "\n",
    "                                # Guardar la ruta del mejor modelo\n",
    "                                best_model_path_temp = fold_model_path\n",
    "\n",
    "    # Mover el mejor modelo a la ubicación final\n",
    "    if best_model_path_temp and best_model_path_temp != best_model_path:\n",
    "        shutil.move(best_model_path_temp, best_model_path)\n",
    "\n",
    "    # Guardar las métricas y los hiperparámetros del mejor modelo\n",
    "    save_metrics_and_params(symbol, model_dir, best_score, best_params)\n",
    "\n",
    "    print(f\"Best parameters for {symbol}: {best_params}\")\n",
    "    print(f\"Best score for {symbol}: {best_score}\")\n",
    "\n",
    "    # Evaluar el mejor modelo\n",
    "    best_model = load_model(best_model_path)\n",
    "    predicted = best_model.predict(X_test)\n",
    "\n",
    "    # Desnormalizar y_test y predicted\n",
    "    y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    predicted_rescaled = target_scaler.inverse_transform(predicted.reshape(-1, 1))\n",
    "\n",
    "    # Calcular métricas\n",
    "    mse = mean_squared_error(y_test_rescaled, predicted_rescaled)\n",
    "    mae = mean_absolute_error(y_test_rescaled, predicted_rescaled)\n",
    "    r2 = r2_score(y_test_rescaled, predicted_rescaled)\n",
    "\n",
    "    print(f\"MSE for {symbol}: {mse}\")\n",
    "    print(f\"MAE for {symbol}: {mae}\")\n",
    "    print(f\"R² for {symbol}: {r2}\")\n",
    "\n",
    "    # Graficar resultados\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test_rescaled, label='Real')\n",
    "    plt.plot(predicted_rescaled, label='Predicho')\n",
    "    plt.title(f'Predicciones de Precios con el Mejor Modelo para {symbol}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the parameters for GridSearch\n",
    "param_grid = {\n",
    "    'units': [[256, 128, 64], [128, 64, 32], [64, 32, 16]],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'recurrent_dropout_rate': [0.2, 0.3],\n",
    "    'optimizer': ['rmsprop'],\n",
    "    'model_type': ['Bidirectional_LSTM'],\n",
    "    'epochs': [30],\n",
    "    'batch_size': [64, 128]\n",
    "}\n",
    "\n",
    "# Función para verificar solapamiento\n",
    "def check_overlap(train_data, test_data):\n",
    "    train_set = set(map(tuple, train_data.reshape((train_data.shape[0], -1))))\n",
    "    test_set = set(map(tuple, test_data.reshape((test_data.shape[0], -1))))\n",
    "    overlap = train_set.intersection(test_set)\n",
    "    return len(overlap) > 0\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "# Define the path to save the logs\n",
    "log_base_dir = '/content/drive/My Drive/tradingcripto/logs'\n",
    "\n",
    "# Define the path to save the models\n",
    "model_dir = os.path.abspath(\"/content/drive/My Drive/tradingcripto/models\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_folds(symbol, sequence_length=50):\n",
    "    # Cargar los datos preparados\n",
    "    prepared_data_path = os.path.join(\"..//content/drive/My Drive/tradingcripto/data/processed\", f\"{symbol}_prepared_data.pkl\")\n",
    "    X_train, X_test, y_train, y_test, feature_scaler, target_scaler = joblib.load(prepared_data_path)\n",
    "\n",
    "    # Configurar la validación cruzada\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, val_index in tscv.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        print(f\"Fold {fold}:\")\n",
    "        print(f\"  Tamaño de X_train_fold: {X_train_fold.shape}\")\n",
    "        print(f\"  Tamaño de X_val_fold: {X_val_fold.shape}\")\n",
    "        print(f\"  Tamaño de y_train_fold: {y_train_fold.shape}\")\n",
    "        print(f\"  Tamaño de y_val_fold: {y_val_fold.shape}\")\n",
    "\n",
    "        # Visualizar los datos\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(y_train_fold, label='y_train_fold')\n",
    "        plt.plot(range(len(y_train_fold), len(y_train_fold) + len(y_val_fold)), y_val_fold, label='y_val_fold')\n",
    "        plt.title(f'Series Temporales para el Fold {fold}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para BTCUSDT\n",
    "train_and_evaluate('BTCUSDT', model_dir, param_grid, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función para analizar los pliegues de BTCUSDT\n",
    "analyze_folds('BTCUSDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ETHUSDT\n",
    "train_and_evaluate('ETHUSDT', model_dir, param_grid, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función para analizar los pliegues de ETHUSDT\n",
    "analyze_folds('ETHUSDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para XRPUSDT\n",
    "train_and_evaluate('XRPUSDT', model_dir, param_grid, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función para analizar los pliegues de XRPUSDT\n",
    "analyze_folds('XRPUSDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para LTCUSDT\n",
    "train_and_evaluate('LTCUSDT', model_dir, param_grid, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función para analizar los pliegues de LTCUSDT\n",
    "analyze_folds('LTCUSDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_params_from_models(model_dir, symbols):\n",
    "    best_params_dict = {}\n",
    "\n",
    "    for symbol in symbols:\n",
    "        symbol_model_dir = os.path.join(model_dir, symbol)\n",
    "        best_model_path = os.path.join(symbol_model_dir, \"best_model.keras\")\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "            # Load the best model\n",
    "            model = load_model(best_model_path)\n",
    "            print(f\"Loaded model for {symbol} from {best_model_path}\")\n",
    "\n",
    "            # Load the best params if they were saved separately\n",
    "            best_params_path = os.path.join(symbol_model_dir, \"best_params.json\")\n",
    "            if os.path.exists(best_params_path):\n",
    "                with open(best_params_path, 'r') as f:\n",
    "                    best_params = json.load(f)\n",
    "                    best_params_dict[symbol] = best_params\n",
    "                    print(f\"Loaded best params for {symbol} from {best_params_path}\")\n",
    "            else:\n",
    "                print(f\"Best params for {symbol} not found.\")\n",
    "        else:\n",
    "            print(f\"Model for {symbol} not found at {best_model_path}\")\n",
    "\n",
    "    return best_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the models\n",
    "model_dir = os.path.abspath(\"/content/drive/My Drive/tradingcripto/models\")\n",
    "symbols = ['BTCUSDT', 'ETHUSDT', 'XRPUSDT', 'LTCUSDT']  # Lista de símbolos a evaluar\n",
    "\n",
    "# Load the best parameters from the saved models\n",
    "best_params_dict = load_best_params_from_models(model_dir, symbols)\n",
    "\n",
    "# Define a new param_grid for fine-tuning each symbol\n",
    "fine_tune_param_grids = {}\n",
    "for symbol, params in best_params_dict.items():\n",
    "    fine_tune_param_grids[symbol] = {\n",
    "        'units': [params['units'] - 10, params['units'], params['units'] + 10],\n",
    "        'dropout_rate': [max(0, params['dropout_rate'] - 0.1), params['dropout_rate'], min(1, params['dropout_rate'] + 0.1)],\n",
    "        'optimizer': [params['optimizer']],\n",
    "        'model_type': [params['model_type']],\n",
    "        'epochs': [60],\n",
    "        'batch_size': [params['batch_size']]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fine-tuning for each symbol\n",
    "for symbol in symbols:\n",
    "    if symbol in fine_tune_param_grids:\n",
    "        print(f\"Fine-tuning model for {symbol}\")\n",
    "        fine_tune_param_grid = fine_tune_param_grids[symbol]\n",
    "        train_and_evaluate(symbol, model_dir, fine_tune_param_grid, sequence_length)\n",
    "    else:\n",
    "        print(f\"No best params found for {symbol}, skipping fine-tuning.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
